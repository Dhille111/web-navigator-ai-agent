# Local AI Agent Configuration

# LLM Configuration (choose one)
LLM_TYPE=ollama  # or gpt4all, llama
# LLM_MODEL_PATH=/path/to/your/model  # for GPT4All or LLaMA
LLM_MODEL_NAME=llama2  # for Ollama

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Browser Configuration
BROWSER_HEADLESS=true
BROWSER_TYPE=chromium

# Memory Configuration
MEMORY_PERSIST=true

# Output Configuration
OUTPUT_DIR=exports

# Flask Configuration
PORT=5000
DEBUG=false
