# Local AI Agent Configuration

# LLM Configuration
LLM_TYPE=gpt4all  # or ollama, llama
LLM_MODEL_PATH=/path/to/your/model
LLM_MODEL_NAME=llama2  # for Ollama

# Ollama Configuration (if using Ollama)
OLLAMA_BASE_URL=http://localhost:11434

# Browser Configuration
BROWSER_HEADLESS=true
BROWSER_TYPE=chromium  # or firefox, webkit

# Memory Configuration
MEMORY_PERSIST=true

# Output Configuration
OUTPUT_DIR=exports

# Flask Configuration
PORT=5000
DEBUG=false
